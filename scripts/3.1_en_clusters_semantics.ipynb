{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in cluster data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "time = 3\n",
    "path = f\"data/en_engagement_clusters_time{time}.csv\"\n",
    "#path = \"data/en_engagement_clusters_full.csv\"\n",
    "en_chirper_data = pd.read_csv(path, usecols=[\"chirper\", \"sample_chirps\", \"cluster\"])\n",
    "#en_chirper_data = pd.read_csv(path, usecols=[\"name\", \"sample_chirps\", \"label\"])\n",
    "en_chirper_data = en_chirper_data.rename(\n",
    "    columns={\"label\": \"cluster\", \"name\": \"chirper\"}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean chirps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_chirper_data.cluster.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Remove chinese characters\n",
    "en_chirper_data[\"sample_chirps\"] = en_chirper_data[\"sample_chirps\"].apply(\n",
    "    lambda x: re.sub(r\"[\\u4e00-\\u9fff]+\", \"\", str(x))\n",
    ")\n",
    "# remove all puctuation\n",
    "en_chirper_data[\"sample_chirps\"] = en_chirper_data[\"sample_chirps\"].apply(\n",
    "    lambda x: re.sub(r\"[^\\w\\s]\", \"\", str(x))\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode chirps and get semantic centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `all-MiniLM-L6-v2` to encode each chirper\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "embeddings = model.encode(\n",
    "    en_chirper_data[\"sample_chirps\"].to_list(), show_progress_bar=True\n",
    ")\n",
    "\n",
    "#with open(f'data/embeddings/engagement_time{time}_chirper_embeddings.pkl', 'wb') as f:\n",
    "with open(f\"data/embeddings/engagement_full_chirper_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f'data/embeddings/engagement_time{time}_chirper_embeddings.pkl', 'rb') as f:\n",
    "#with open(f\"data/embeddings/engagement_full_chirper_embeddings.pkl\", \"rb\") as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "cluster_semantic_centroids = pd.DataFrame(\n",
    "    embeddings\n",
    ")\n",
    "cluster_semantic_centroids[\"cluster\"] = en_chirper_data[\"cluster\"].to_list()\n",
    "cluster_semantic_centroids[\"chirper\"] = en_chirper_data[\"chirper\"].to_list()\n",
    "cluster_semantic_centroids = cluster_semantic_centroids.groupby(\"cluster\").mean()\n",
    "\n",
    "global_semantic_centroid = embeddings.mean(axis=0)\n",
    "\n",
    "all_centroids = pd.concat(\n",
    "    [cluster_semantic_centroids, pd.DataFrame([global_semantic_centroid])], axis=0\n",
    ")\n",
    "\n",
    "all_centroids.index = [\"cluster_\" + str(i) for i in all_centroids.index[:-1]] + [\n",
    "    \"global\"\n",
    "]\n",
    "\n",
    "#all_centroids.to_csv(f\"data/embeddings/engagement_time{time}_cluster_centroids.csv\")\n",
    "#all_centroids.to_csv(f\"data/embeddings/engagement_full_cluster_centroids.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get semantic distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "for idx, chirper in enumerate(en_chirper_data[\"chirper\"].to_list()):\n",
    "    their_cluster = en_chirper_data.loc[idx, \"cluster\"]\n",
    "    en_chirper_data.loc[idx, \"semantic_distance_to_cluster\"] = cosine(\n",
    "        embeddings[idx], cluster_semantic_centroids.loc[their_cluster]\n",
    "    )\n",
    "    en_chirper_data.loc[idx, \"semantic_distance_to_global\"] = cosine(\n",
    "        embeddings[idx], global_semantic_centroid\n",
    "    )\n",
    "    if idx % 1000 == 0: print(idx, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data for analysis in R\n",
    "#en_chirper_data.to_csv(f\"data/networks/en_engagement_clusters_time{time}.csv\", index=False)\n",
    "en_chirper_data.to_csv(f\"data/networks/en_engagement_clusters_full.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a cluster data frame that has:\n",
    "# - cluster\n",
    "# - cluster size\n",
    "# - cluster key words - join all the chirpers' key words\n",
    "\n",
    "cluster_data = en_chirper_data.groupby(\"cluster\").agg(\n",
    "    {\n",
    "        \"chirper\": \"count\",\n",
    "        \"sample_chirps\": lambda x: \" \".join(x),\n",
    "    }\n",
    ")\n",
    "cluster_data = cluster_data.rename(columns={\"chirper\": \"cluster_size\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tf-idf to get the top 3 words for each cluster, concatenate into a label\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix: csr_matrix = vectorizer.fit_transform(cluster_data[\"sample_chirps\"])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "\n",
    "def get_key_words(i, tfidf_matrix, feature_names):\n",
    "    cluster_document = tfidf_matrix[i]\n",
    "    non_zero_indices = cluster_document.nonzero()[1]\n",
    "    tfidf_scores = zip(feature_names[non_zero_indices], cluster_document.data)\n",
    "    sorted_tfidf_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
    "    key_words = [word for word, score in sorted_tfidf_scores[:3]]\n",
    "    return \" \".join(key_words)\n",
    "\n",
    "\n",
    "# extract the top 3 words for each cluster\n",
    "num_clusters = len(cluster_data)\n",
    "cluster_data[\"top_3_words\"] = [get_key_words(i, tfidf_matrix, feature_names) for i in range(num_clusters)]\n",
    "\n",
    "#cluster_data.to_csv(f\"data/networks/en_engagement_clusters_time{time}_cluster_data.csv\")\n",
    "cluster_data.to_csv(f\"data/networks/en_engagement_clusters_full_cluster_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use `UMAP` for dimensionality reduction to visualise clusters in 2D\n",
    "import umap\n",
    "\n",
    "chirper_embeddings_2d = umap.UMAP(\n",
    "    n_neighbors=15, n_components=2, metric=\"cosine\"\n",
    ").fit_transform(embeddings)\n",
    "chirper_embeddings_2d = pd.DataFrame(chirper_embeddings_2d)\n",
    "\n",
    "chirper_embeddings_2d[\"cluster\"] = en_chirper_data[\"cluster\"].to_list()\n",
    "chirper_embeddings_2d[\"chirper\"] = en_chirper_data[\"chirper\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot clusters in 2D\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "global_centroid_2d = chirper_embeddings_2d.iloc[:, :-2].mean(axis=0).to_list()\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(9, 9))\n",
    "plt.scatter(\n",
    "    chirper_embeddings_2d.iloc[:, 0],\n",
    "    chirper_embeddings_2d.iloc[:, 1],\n",
    "    # colour by cluster\n",
    "    c=chirper_embeddings_2d[\"cluster\"],\n",
    "    cmap=\"tab20\",\n",
    "    alpha=0.8,\n",
    "    s=5,\n",
    ")\n",
    "\n",
    "plt.title(f\"Semantic Distribution at Time {time}\")\n",
    "#plt.title(f\"Distribution of full engagement communities' semantic centroids\")\n",
    "plt.savefig(f\"nlp_results/engagement_time{time}_semantic_distribution.png\")\n",
    "#plt.savefig(f\"nlp_results/engagement_full_semantic_clusters.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# produce a word cloud for some chosen clusters\n",
    "from wordcloud import WordCloud\n",
    "chosen_clusters = [1, 2, 3, 4]\n",
    "num_chirpers = len(en_chirper_data)\n",
    "for cluster in chosen_clusters:\n",
    "    wordcloud = WordCloud(\n",
    "        background_color=\"white\",\n",
    "        width=1000,\n",
    "        height=500,\n",
    "        max_words=50,\n",
    "        contour_width=3,\n",
    "        contour_color=\"steelblue\",\n",
    "    ).generate(cluster_data.loc[cluster, \"sample_chirps\"])\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    cluster_size = cluster_data.loc[cluster, \"cluster_size\"]\n",
    "    plt.title(\n",
    "        #f\"English Engagement Network T{time} Cluster {cluster} | N = {cluster_size} [{cluster_size/num_chirpers:.2%}]\"\n",
    "        f\"English Engagement Network Full Cluster {cluster} | N = {cluster_size} [{cluster_size/num_chirpers:.2%}]\"\n",
    "    )\n",
    "    #plt.savefig(f\"nlp_results/engagement_time{time}_cluster{cluster}_wordcloud.png\")\n",
    "    plt.savefig(f\"nlp_results/engagement_full_cluster{cluster}_wordcloud.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get cluster distance summary stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time = 3\n",
    "#path = f\"data/embeddings/engagement_time{time}_cluster_centroids.csv\"\n",
    "path = \"data/embeddings/engagement_full_cluster_centroids.csv\"\n",
    "cluster_centroids = pd.read_csv(path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine distance from each cluster to the global centroid\n",
    "from scipy.spatial.distance import cosine\n",
    "from numpy import mean, std\n",
    "\n",
    "\n",
    "distances_to_global = cluster_centroids.apply(\n",
    "    lambda x: cosine(x, cluster_centroids.loc[\"global\"]), axis=1\n",
    ")\n",
    "print(mean(distances_to_global), std(distances_to_global))\n",
    "\n",
    "# get confidence interval\n",
    "from scipy.stats import t\n",
    "\n",
    "confidence = 0.95\n",
    "n = len(distances_to_global)\n",
    "m = mean(distances_to_global)\n",
    "std_err = std(distances_to_global) / n ** 0.5\n",
    "h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "\n",
    "print(m - h, m + h)\n",
    "\n",
    "# do a t-test to see if higher than 0,\n",
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "data = distances_to_global\n",
    "stat, p = ttest_1samp(data, 0)\n",
    "d = (mean(data) - 0) / (std(data))\n",
    "\n",
    "print(\"Statistics=%.3f, p=%.3f\" % (stat, p))\n",
    "print(\"Cohen's d=%.3f\" % (d))\n",
    "\n",
    "len(distances_to_global)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T1: N(Cluster) = 35\n",
    "- Mean, SD: 0.06579766958060736 0.03724406952116002\n",
    "- 95%CI: 0.05300388407331168 0.07859145508790304\n",
    "- Statistics=10.301, p=0.000\n",
    "- Cohen's d=1.767\n",
    "\n",
    "T2: N(Cluster) = 20\n",
    "- 0.016283563254110643 0.008962746575326903\n",
    "- 0.012088868735766947 0.02047825777245434\n",
    "- Statistics=7.919, p=0.000\n",
    "- Cohen's d=1.817\n",
    "\n",
    "T3: N(Cluster) = 12\n",
    "- 0.028265152586819293 0.019004585208362488\n",
    "- 0.016190215225181194 0.04034008994845739\n",
    "- Statistics=4.933, p=0.000\n",
    "- Cohen's d=1.487\n",
    "\n",
    "Full: N = 4\n",
    "- 0.03697118019196259 0.019783590353789918\n",
    "- 0.012406608294450335 0.06153575208947484\n",
    "- Statistics=3.738, p=0.020\n",
    "- Cohen's d=1.869"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chirper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
